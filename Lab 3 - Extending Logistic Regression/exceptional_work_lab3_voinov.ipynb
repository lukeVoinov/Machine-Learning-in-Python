{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FQWe8wgx75J",
        "outputId": "b4d48f3d-8b81-4f03-f7eb-b744c21306aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.6.0)\n",
            "Requirement already satisfied: numpy<2.6,>=1.22 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install cupy-cuda12x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8PiygHyLotRU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import cupy as cp\n",
        "import numpy as np_cpu\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqp4yyRAo5Gq",
        "outputId": "a077013d-774b-4d83-ddee-40fe5f69c34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (21407, 1126)\n",
            "Test set shape: (5352, 1126)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "path = \"f1_cleaned.csv\"\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Features and target variable\n",
        "X = df.drop(columns=[\"podium\", \"positionOrder\"])\n",
        "y = df[\"podium\"]\n",
        "\n",
        "# For now, we'll encode categorical variables using one-hot encoding\n",
        "X = pd.get_dummies(X, columns=[\"raceName\", \"driverName\", \"constructorName\"], drop_first=True)\n",
        "\n",
        "# Train-test split with stratification (80% train, 20% split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df[\"podium\"]) # stratify to maintain class distribution\n",
        "\n",
        "# Convert to numpy arrays with proper data types\n",
        "X_train = X_train.astype(np.float32).values\n",
        "X_test = X_test.astype(np.float32).values\n",
        "y_train = y_train.astype(np.int32).values\n",
        "y_test = y_test.astype(np.int32).values\n",
        "\n",
        "# Scale the array\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training set shape:\", X_train_scaled.shape)\n",
        "print(\"Test set shape:\", X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GlQUoy1Aklu5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma # Used for masked array log operations in objective function (optional, but robust)\n",
        "from scipy.special import expit\n",
        "\n",
        "class CPULogisticRegression:\n",
        "    # private:\n",
        "    def __init__(self, eta=0.01, iterations=20, C=0.0):\n",
        "        self.eta = eta\n",
        "        self.iters = iterations\n",
        "        self.C = C\n",
        "        # internally we will store the weights as self.w_\n",
        "        self.w_ = None\n",
        "        self.classifiers_ = []\n",
        "        self.unique_ = None\n",
        "\n",
        "    def __str__(self):\n",
        "        if(hasattr(self,'w_') and self.w_ is not None):\n",
        "            return 'MultiClass Steepest Ascent Logistic Regression Object (Trained)'\n",
        "        else:\n",
        "            return 'Untrained MultiClass Steepest Ascent Logistic Regression Object'\n",
        "\n",
        "    @property\n",
        "    def coef_(self):\n",
        "        if(hasattr(self,'w_') and self.w_ is not None):\n",
        "            return self.w_[:,1:]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @property\n",
        "    def intercept_(self):\n",
        "        if(hasattr(self,'w_') and self.w_ is not None):\n",
        "            return self.w_[:,0]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Core Binary Solver: Steepest Ascent Regression (Fixed)\n",
        "    # ----------------------------------------------------\n",
        "    def SAfit(self, X, y):\n",
        "        \"\"\"Performs Steepest Ascent (Gradient Ascent) for a single binary problem.\"\"\"\n",
        "        Xb = self._add_intercept(X)\n",
        "        num_samples, num_features = Xb.shape\n",
        "\n",
        "        # Initialize weights (specific to this binary fit instance)\n",
        "        w_binary = np.zeros((num_features, 1))\n",
        "\n",
        "        # Steepest Ascent loop\n",
        "        for _ in range(self.iters):\n",
        "            # Calculate gradient (must pass required state: Xb, y, w_binary)\n",
        "            gradient = self._calculate_gradient(Xb, y, w_binary, self.C)\n",
        "            w_binary += self.eta * gradient # gradient ascent\n",
        "\n",
        "        # Store the final weights for this binary classifier instance\n",
        "        self.w_ = w_binary\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Multi-Class Training Method (Fixed)\n",
        "    # ----------------------------------------------------\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Trains the multi-class model using One-vs-Rest strategy.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        self.unique_ = np.unique(y) # Stores unique class labels\n",
        "        self.classifiers_ = [] # Reset classifiers list\n",
        "\n",
        "        # Train one binary classifier (SAfit) for each unique class (One-vs-Rest)\n",
        "        for yval in self.unique_:\n",
        "            y_binary = (y == yval).astype(int) # Create 0/1 binary labels\n",
        "\n",
        "            # Create a NEW instance of the classifier for the binary problem\n",
        "            blr_instance = CPULogisticRegression(eta=self.eta, iterations=self.iters, C=self.C)\n",
        "\n",
        "            # Train the binary classifier using SAfit\n",
        "            # Note: X is passed without intercept here, as SAfit adds it internally\n",
        "            blr_instance.SAfit(X, y_binary)\n",
        "\n",
        "            # Store the trained instance\n",
        "            self.classifiers_.append(blr_instance)\n",
        "\n",
        "        # Combine all binary classifier weights into one large matrix\n",
        "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Gradient Calculation (Fixed and Simplified)\n",
        "    # ----------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _calculate_gradient(Xb, y, w, C):\n",
        "        \"\"\"\n",
        "        Calculates the regularized gradient for Steepest Ascent.\n",
        "        Note: Xb includes the intercept term.\n",
        "        \"\"\"\n",
        "        # Calculate probability: g = sigmoid(Xb * w)\n",
        "        g = expit(Xb @ w)\n",
        "\n",
        "        # ydiff = y - g\n",
        "        ydiff = y.reshape(g.shape) - g\n",
        "\n",
        "        # Unregularized gradient: Xb.T @ ydiff / num_samples\n",
        "        gradient = Xb.T @ ydiff / Xb.shape[0]\n",
        "\n",
        "        # Add L2 regularization term to all features (excluding the intercept at index 0)\n",
        "        gradient[1:] -= 2 * C * w[1:]\n",
        "\n",
        "        return gradient\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Convenience and Prediction Methods (Fixed)\n",
        "    # ----------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _sigmoid(theta):\n",
        "        return expit(theta)\n",
        "\n",
        "    @staticmethod\n",
        "    def _add_intercept(X):\n",
        "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
        "\n",
        "    def predict_proba(self,X):\n",
        "        \"\"\"Calculates probabilities for all classes.\"\"\"\n",
        "        Xb = self._add_intercept(X) # Add intercept to prediction data\n",
        "        probs = []\n",
        "\n",
        "        if not self.classifiers_:\n",
        "             raise RuntimeError(\"Model must be fitted before calling predict_proba.\")\n",
        "\n",
        "        for blr in self.classifiers_:\n",
        "            # Retrieve weights from the trained binary instance\n",
        "            w_binary = blr.w_\n",
        "\n",
        "            # Calculate P(Y=yval) = sigmoid(Xb * w)\n",
        "            theta = Xb @ w_binary\n",
        "            prob = self._sigmoid(theta)\n",
        "            probs.append(prob)\n",
        "\n",
        "        return np.hstack(probs) # Stack into a single matrix (N_samples x N_classes)\n",
        "\n",
        "    def predict(self,X):\n",
        "        \"\"\"Returns the class label with the highest probability.\"\"\"\n",
        "        probs = self.predict_proba(X)\n",
        "        max_indices = np.argmax(probs, axis=1) # Get the index of the max probability\n",
        "\n",
        "        # Look up the class label using the unique classes array\n",
        "        return self.unique_[max_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "t1cWBCGrqGiq"
      },
      "outputs": [],
      "source": [
        "# I used google gemini here to help out the gpu implementation\n",
        "# The main difference here is that the GPU implementation uses cupy, which is like numpy but for GPUs\n",
        "\n",
        "class GPULogisticRegression:\n",
        "\n",
        "    def __init__(self, eta=0.01, iterations=20, C=0.0):\n",
        "        self.eta = eta\n",
        "        self.iters = iterations\n",
        "        self.C = C\n",
        "        self.w_ = None\n",
        "        self.classifiers_ = []\n",
        "        self.unique_ = None\n",
        "\n",
        "    def __str__(self):\n",
        "        if(hasattr(self,'w_') and isinstance(self.w_, cp.ndarray)):\n",
        "            return 'MultiClass Logistic Regression Object (CuPy-Accelerated) with coefficients:\\n' + str(self.w_.get())\n",
        "        else:\n",
        "            return 'Untrained MultiClass Logistic Regression Object (CuPy-Accelerated)'\n",
        "\n",
        "    @property\n",
        "    def coef_(self):\n",
        "        if(hasattr(self,'w_') and isinstance(self.w_, cp.ndarray)):\n",
        "            return self.w_[:,1:].get()\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def intercept_(self):\n",
        "        if(hasattr(self,'w_') and isinstance(self.w_, cp.ndarray)):\n",
        "            return self.w_[:,0].get()\n",
        "        return None\n",
        "\n",
        "    def SAfit(self, X, y):\n",
        "        Xb = self._add_intercept(X)\n",
        "        num_samples, num_features = Xb.shape\n",
        "\n",
        "        self.w_ = cp.zeros((num_features, 1), dtype=Xb.dtype)\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            gradient = self._calculate_gradient(Xb, y, self.w_, self.C)\n",
        "\n",
        "            self.w_ += self.eta * gradient\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if not isinstance(X, cp.ndarray):\n",
        "             X_gpu = cp.asarray(X)\n",
        "        else:\n",
        "             X_gpu = X\n",
        "\n",
        "        if not isinstance(y, cp.ndarray):\n",
        "             y_gpu = cp.asarray(y)\n",
        "        else:\n",
        "             y_gpu = y\n",
        "\n",
        "        self.unique_ = cp.unique(y_gpu)\n",
        "        self.classifiers_ = []\n",
        "\n",
        "        for yval in self.unique_:\n",
        "            y_binary = (y_gpu == yval).astype(X_gpu.dtype)\n",
        "            blr = GPULogisticRegression(eta=self.eta, iterations=self.iters, C=self.C)\n",
        "            blr.SAfit(X_gpu, y_binary)\n",
        "\n",
        "            self.classifiers_.append(blr)\n",
        "\n",
        "        self.w_ = cp.hstack([x.w_ for x in self.classifiers_])\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(theta):\n",
        "        return expit(theta)\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_gradient(Xb, y, w, C):\n",
        "        g = expit(Xb @ w)\n",
        "\n",
        "        y_reshaped = y.reshape(g.shape)\n",
        "        ydiff = y_reshaped - g\n",
        "\n",
        "        gradient = Xb.T @ ydiff / Xb.shape[0]\n",
        "\n",
        "        # Add L2 regularization\n",
        "        gradient[1:] -= 2 * C * w[1:]\n",
        "\n",
        "        return gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def _add_intercept(X):\n",
        "        return cp.hstack((cp.ones((X.shape[0], 1), dtype=X.dtype), X))\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not isinstance(X, cp.ndarray):\n",
        "             X_gpu = cp.asarray(X)\n",
        "        else:\n",
        "             X_gpu = X\n",
        "\n",
        "        Xb = self._add_intercept(X_gpu)\n",
        "        probs = []\n",
        "\n",
        "\n",
        "        for blr in self.classifiers_:\n",
        "            theta = Xb @ blr.w_\n",
        "            prob = self._sigmoid(theta)\n",
        "            probs.append(prob)\n",
        "\n",
        "        return cp.hstack(probs).get()\n",
        "\n",
        "    def predict(self,X):\n",
        "        probs_cpu = self.predict_proba(X)\n",
        "\n",
        "        max_indices = cp.argmax(cp.asarray(probs_cpu), axis=1).get()\n",
        "\n",
        "        return self.unique_.get()[max_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tX_3cFJyAOV",
        "outputId": "4e432e4c-85ed-4567-b79e-e422be61fb0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU Accuracy: 0.8729446935724963\n",
            "CPU times: user 11.5 s, sys: 163 ms, total: 11.6 s\n",
            "Wall time: 6.46 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# CPU time\n",
        "cpu_lr = CPULogisticRegression(eta=0.1389,iterations=50,C=0.1389)\n",
        "cpu_lr.fit(X_train_scaled,y_train)\n",
        "\n",
        "cpu_yhat = cpu_lr.predict(X_test_scaled)\n",
        "cpu_acc = accuracy_score(y_test,cpu_yhat)\n",
        "print(\"CPU Accuracy:\", cpu_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv7cU5R_zASC",
        "outputId": "f886d11c-f028-4a69-94b0-f38bcf9ca368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Accuracy: 0.8729446935724963\n",
            "CPU times: user 220 ms, sys: 16 ms, total: 236 ms\n",
            "Wall time: 236 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# GPU time\n",
        "gpu_lr = GPULogisticRegression(eta=0.1389,iterations=50,C=0.1389)\n",
        "gpu_lr.fit(X_train_scaled,y_train)\n",
        "\n",
        "gpu_yhat = gpu_lr.predict(X_test_scaled)\n",
        "gpu_acc = accuracy_score(y_test,gpu_yhat)\n",
        "print(\"GPU Accuracy:\", gpu_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaszhlXW1Db5"
      },
      "source": [
        "As we can see, the GPU implementation on Steepest Ascent with L2 norm is much faster (27 times faster) relative to the CPU version. They achieve the same accuracy because nothing in their code changes. The only difference is that one of them uses a CPU while the other one exploits the GPU. Notice that the GPU not only has quicker wall time but also quicker CPU times (user, sys, and total are in ms whereas the CPU has these in seconds).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

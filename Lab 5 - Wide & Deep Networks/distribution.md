# **Points/Work Distrbution for Lab 5 - Wide & Deep Networks**
3 points are distributed among the team members: Nimai, Emmanuel, Luke, and Tiffany. Some parts may overlap and may need more support from rest of team members. If one person is done with their part, they should be available to help out with other people's parts. 

### **Preparation** (4.5 points total)
- [ ] [1 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used (include a description of any newly formed variables you created). It is preferred to use tf.dataset and Keras Feature spaces for pre-processing, but it is not required. 
- [ ] [1.5 points] Identify groups of features in your data that should be combined into cross-product features. Provide a compelling justification for why these features should be crossed (or, alternatively, why some features should not be crossed). 
- [ ] [1 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use, but sometimes it can be. Think critically about an appropriate measure of performance and explain this reasoning. Also include a discussion of bananas.
- [ ] [1 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate and use this method during training. Argue why your splitting method is a realistic mirroring of how an algorithm would be used in practice.  

### **Modeling** (5.5 points total)
- [ ] [2 points] Create at least three (Total) wide and deep networks to classify your data using Keras (this total of "three" includes the model you will train in the next step of the rubric). Visualize the performance of the network on the training data and validation data in the same plot versus the training iterations. Be sure that each model has converged, perhaps using a log-scaled loss plot. 
Note: you can use the "history" return parameter that is part of Keras "fit" function to easily access this data.
For each model, you should include a discussion of the number of trainable parameters and briefly describe the characteristics of each model. 
- [ ] [2 points] Investigate generalization performance by altering the number of layers in the deep branch of the network. Try at least two models (this "two" includes the wide and deep model trained from the previous step). For a third model, change the way you are processing the input feature data or the way you are crossing the data. You have free rein to choose how you want to change the pre-processing.  Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab to answer: What model with what number of layers performs superiorly? Use proper statistical methods to compare the performance.
- [ ] [1.5 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP). Alternatively, you can compare to a network without the wide branch (i.e., just the deep network). Include a comparison using the receiver operating characteristic and area under the curve (ROC visual and AUROC).   Use proper statistical methods to compare the performance of different models and discuss the results.  

### **Exceptional Work** (1 point total)
- [ ] One idea (required for 7000 level students): Capture the embedding weights from the deep network and (if needed) perform dimensionality reduction on the output of these embedding layers (only if needed). That is, pass the observations into the network, save the embedded weights (called embeddings), and then perform  dimensionality reduction in order to visualize results. Visualize and explain any clusters in the data.
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ff1f2ff",
      "metadata": {
        "id": "9ff1f2ff"
      },
      "source": [
        "# **Lab 4: Multi-Layer Perceptron**\n",
        "**Name(s):** Luke Voinov, Tiffany Nguyen, Emmanuel Garcia, Nimai Keshu\n",
        "\n",
        "We use the US Census dataset:\n",
        "\n",
        "https://www.kaggle.com/muonneutrino/us-census-demographic-data/data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0850a0c4",
      "metadata": {
        "id": "0850a0c4"
      },
      "source": [
        "##### **Classification Task:**\n",
        "\n",
        "The classification task you will be performing is to predict, **for each tract, what the child poverty rate will be**. You will need to convert this from regression to four levels of classification by quantizing the variable of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8180fe38",
      "metadata": {
        "id": "8180fe38"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import any dependencies\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.special import expit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9d9c9f",
      "metadata": {
        "id": "7a9d9c9f"
      },
      "source": [
        "### **1. Load, Split, and Balance**\n",
        "\n",
        "*1.1. Load the data into memory and save it to a pandas data frame. Do not normalize or one-hot encode any of the features until asked to do so later in the rubric.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7ff15f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "fd7ff15f",
        "outputId": "1cb337bc-4a3b-4ee3-f2a1-5ee83b26bde9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/acs2017_census_tract_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3655244932.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/acs2017_census_tract_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/acs2017_census_tract_data.csv'"
          ]
        }
      ],
      "source": [
        "path = \"/content/acs2017_census_tract_data.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc850200",
      "metadata": {
        "id": "cc850200"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02279d6",
      "metadata": {
        "id": "d02279d6"
      },
      "source": [
        "*1.2 Remove any observations that having missing data.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752d10cb",
      "metadata": {
        "id": "752d10cb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Visualize missing data\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', DeprecationWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "# External package: conda install missingno\n",
        "import missingno as mn\n",
        "\n",
        "mn.matrix(df)\n",
        "plt.title(\"Missing Data Visual\",fontsize=22)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa7e4c7a",
      "metadata": {
        "id": "fa7e4c7a"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())  # Shows the count of missing values per column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8294a50e",
      "metadata": {
        "id": "8294a50e"
      },
      "source": [
        "We can see that no column is missing too much data. Therefore, it is okay to leave these missing values as they are; imputation is not necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4326a393",
      "metadata": {
        "id": "4326a393"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Remove all rows that have a population of less than 100 people.\n",
        "Delete any rows that contain missing data\n",
        "\"\"\"\n",
        "# keep rows with TotalPop >= 100, reset index\n",
        "df_mod = df[df['TotalPop'] >= 100].reset_index(drop=True)\n",
        "\n",
        "# also delete any row that has any missing value\n",
        "df_clean = df_mod.dropna().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe68d28",
      "metadata": {
        "id": "1fe68d28"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Visualize missing data\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', DeprecationWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "# External package: conda install missingno\n",
        "import missingno as mn\n",
        "\n",
        "mn.matrix(df_clean)\n",
        "plt.title(\"Missing Data Visual\",fontsize=22)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df_clean.count().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fc5249e",
      "metadata": {
        "id": "4fc5249e"
      },
      "source": [
        "We started with 74001 and ended with 72708 people. This means that deleting every row with a missing value deleted ~2% of the data. This is small and permits deletion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6078a2",
      "metadata": {
        "id": "ee6078a2"
      },
      "source": [
        "*1.3 Encode any string data as integers for now.*\n",
        "\n",
        "We have already seen that there are only two columns that contain strings: the state and the county"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f77c0f",
      "metadata": {
        "id": "f7f77c0f"
      },
      "outputs": [],
      "source": [
        "# encode states\n",
        "states = df_clean['State'].unique()\n",
        "count = 0\n",
        "\n",
        "df_state_enc = df_clean\n",
        "for s in states:\n",
        "    df_state_enc = df_state_enc.replace(s,count)\n",
        "    count += 1\n",
        "\n",
        "print(\"States encoded to ints:\\n\", df_state_enc['State'].unique())\n",
        "\n",
        "# encode counties\n",
        "counties = df_clean['County'].unique()\n",
        "count = 0\n",
        "\n",
        "df_enc = df_state_enc\n",
        "for c in counties:\n",
        "    df_enc = df_enc.replace(c,count)\n",
        "    count += 1\n",
        "\n",
        "print(\"Counties encoded to ints:\\n\", df_enc['County'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea165d0",
      "metadata": {
        "id": "cea165d0"
      },
      "outputs": [],
      "source": [
        "print(df_enc.info(verbose=False)) # Notice the dtypes\n",
        "df_enc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e99417",
      "metadata": {
        "id": "68e99417"
      },
      "source": [
        "1.4 You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable.\n",
        "\n",
        "We decided to keep 'county' because we suspect each county will meaningfully inform child poverty rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991ed33c",
      "metadata": {
        "id": "991ed33c"
      },
      "outputs": [],
      "source": [
        "# see how many instances there are for each county\n",
        "counties = df_enc['County'].unique()\n",
        "count = 0\n",
        "num_each_instance = []\n",
        "\n",
        "for c in counties:\n",
        "    bools = df_enc['County'] == c\n",
        "    num_each_instance.append(df_enc.where(bools == True)['County'].count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc91a91c",
      "metadata": {
        "id": "bc91a91c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(num_each_instance,bins=200)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.boxplot(num_each_instance)\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean:\", np.mean(num_each_instance))\n",
        "print(\"Median:\", np.median(num_each_instance))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a02b8b",
      "metadata": {
        "id": "e5a02b8b"
      },
      "source": [
        "Here, as we can see from the graphs, a median is a more representative value due to the tremendous effect of outliers. This median shows that each county has around 9 census tracts, and each census tract has around 1k people. While all the counties are in the same state with the same laws and policies, they can still vary dramatically between each other. As an anecdote, it would be reasonable to suspect that child poverty rates are lower in the Highland Park county as compared to the Oak Cliff county.\n",
        "\n",
        "We conclude our decision with a PCA analysis that demonstrate how important of a feature counties are: **CAN I DO THIS**?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba8b633",
      "metadata": {
        "id": "bba8b633"
      },
      "source": [
        "The next two requirements will need to be completed together as they might depend on one another.\n",
        "\n",
        "*Note: You will need to one hot encode the target, but **do not** one hot encode the categorical data **until** instructed to do so in the lab.*\n",
        "\n",
        "1.5 Balance the dataset so that about the same number of instances are within each class. Choose a method for balancing the dataset and explain your reasoning for selecting this method. One option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into four classes. *Should balancing of the dataset be done for both the training and testing set? Explain.*\n",
        "\n",
        "We balanced the dataset according to 4 percentiles: 75% > in poverty level = impoverished, 75 - 50 % are poor, 50 - 25 % are well-off, 25 % < are rich. All of these results will be relative to the training data and will equally split 1/4 of the dataset into each class.\n",
        "\n",
        "The training dataset must be balanced because it will allow us to categorize among the 4 classes. The test dataset should also be balanced to reflect the real-world use case. If it's unbalanced, then a split test case may result in 99% impoverished instances and 1% of the rest, which is inapproprite if we want to test real-life scenarios.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee48e6f",
      "metadata": {
        "id": "aee48e6f"
      },
      "outputs": [],
      "source": [
        "# This code is adapted from github Copilot.\n",
        "# It will split the dataset into 4 quartiles and place 25% of the dataset into a quartile for each quartile, thus evenly distributing the data.\n",
        "\n",
        "\n",
        "q25 = df_enc['ChildPoverty'].quantile(0.25)\n",
        "q50 = df_enc['ChildPoverty'].quantile(0.50)\n",
        "q75 = df_enc['ChildPoverty'].quantile(0.75)\n",
        "\n",
        "# Create balanced classes\n",
        "def classify_poverty(value):\n",
        "    if value <= q25:\n",
        "        return 0  # rich\n",
        "    elif value <= q50:\n",
        "        return 1  # well-off\n",
        "    elif value <= q75:\n",
        "        return 2  # poor\n",
        "    else:\n",
        "        return 3  # impoverished\n",
        "\n",
        "df_enc['PovertyClass'] = df_enc['ChildPoverty'].apply(classify_poverty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6389aae",
      "metadata": {
        "id": "b6389aae"
      },
      "outputs": [],
      "source": [
        "df_enc['PovertyClass']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4a77c5",
      "metadata": {
        "id": "9c4a77c5"
      },
      "source": [
        "1.6 Assume you are equally interested in the classification performance for each class in the dataset. Split the dataset into 80% for training and 20% for testing. There is no need to split the data multiple times for this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f3ba15",
      "metadata": {
        "id": "d2f3ba15"
      },
      "outputs": [],
      "source": [
        "\n",
        "# This code is adapted from voinov_lab3\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop irrelevent columns and classes from feature list\n",
        "X = df_enc.drop(columns=[\"TractId\", \"ChildPoverty\", \"PovertyClass\"])\n",
        "y = df_enc[\"ChildPoverty\"]\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Previously the quantile included all the data. Now it has to only include 80% of the data. The test quantiles will be the same as the train one to be realistic.\n",
        "q25 = y_train.quantile(0.25)\n",
        "q50 = y_train.quantile(0.50)\n",
        "q75 = y_train.quantile(0.75)\n",
        "\n",
        "# Now make the classes\n",
        "y_train = y_train.apply(classify_poverty).values\n",
        "y_test = y_test.apply(classify_poverty).values\n",
        "\n",
        "# Convert to numpy arrays with proper data types. Provided by github copilot to fix type issues\n",
        "X_train = X_train.astype(np.float32).values\n",
        "X_test = X_test.astype(np.float32).values\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nTraining set shape:\", X_train_scaled.shape)\n",
        "print(\"Test set shape:\", X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8395e812",
      "metadata": {
        "id": "8395e812"
      },
      "outputs": [],
      "source": [
        "# LOOK: this is from someone else's lab, works with Tiffany's code\n",
        "# so we may need to do some revisions in terms of prepping the data\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "X = df_enc.drop(columns=[\"PovertyClass\"]) # features\n",
        "y = df_enc[\"PovertyClass\"] # target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "counts_train = Counter(y_train)\n",
        "min_size = min(counts_train.values())\n",
        "\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_train_balanced = (df_train.groupby('PovertyClass')).apply(lambda x: x.sample(min_size, random_state=42)).reset_index(drop=True)\n",
        "\n",
        "X_train_balanced = df_train_balanced.drop(columns=[\"PovertyClass\"])\n",
        "y_train_balanced = df_train_balanced[\"PovertyClass\"]\n",
        "\n",
        "print(\"\\nFinal Shapes:\")\n",
        "print(\"X_train_balanced:\", X_train_balanced.shape)\n",
        "print(\"y_train_balanced:\", y_train_balanced.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_test:\", y_test.shape)\n",
        "\n",
        "y_train = y_train_balanced\n",
        "X_train = X_train_balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bc01b8",
      "metadata": {
        "id": "75bc01b8"
      },
      "source": [
        "### **2. Pre-processing and Initial Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16fa360f",
      "metadata": {
        "id": "16fa360f"
      },
      "source": [
        "2.1  You will be using a two layer perceptron from class for the next few parts of the rubric. There are several versions of the two layer perceptron covered in class, with example code. When selecting an example two layer network from class be sure that you use: (1) vectorized gradient computation, (2) mini-batching, (3) cross entropy loss, and (4) proper Glorot initialization, at a minimum. There is no need to use momentum or learning rate reduction (assuming you choose a sufficiently small learning rate). **It is recommended to use sigmoids throughout the network, but not required**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6854329",
      "metadata": {
        "id": "b6854329"
      },
      "source": [
        "2.2 Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Do not normalize or one-hot encode the feature data (not yet). Be sure that training converges by graphing the loss function versus the number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca372dae",
      "metadata": {
        "id": "ca372dae"
      },
      "source": [
        "This is based off of the Two Layer Perceptron from Eric Larson's version from the class. We modified it so that we would be able to use minibatches_size as one of the parameters and made it suitable for multi-class classification by using softmax. The mathematics behind is consistently the same as the one in class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ff207a",
      "metadata": {
        "id": "61ff207a"
      },
      "outputs": [],
      "source": [
        "# Imported from 07. MLP Neural Networks.ipynb (Eric Larson, CS 5324/7324)\n",
        "\n",
        "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
        "# Original Author: Sebastian Raschka\n",
        "\n",
        "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
        "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
        "\n",
        "class TwoLayerPerceptron:\n",
        "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, minibatch_size=64, random_state=None):\n",
        "        np.random.seed(random_state)\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2_C = C\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.minibatch_size = minibatch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_labels(y):\n",
        "        \"\"\"Encode labels into one-hot representation.\"\"\"\n",
        "        return pd.get_dummies(y).values.T\n",
        "\n",
        "    def _initialize_weights(self, n_features, n_output):\n",
        "        \"\"\"Initialize weights using Glorot initialization.\"\"\"\n",
        "        limit_1 = np.sqrt(6 / (n_features + self.n_hidden))\n",
        "        limit_2 = np.sqrt(6 / (self.n_hidden + n_output))\n",
        "\n",
        "        W1 = np.random.uniform(-limit_1, limit_1, (self.n_hidden, n_features + 1))  # +1 for bias term\n",
        "        W2 = np.random.uniform(-limit_2, limit_2, (n_output, self.n_hidden + 1))  # +1 for bias term\n",
        "        b1 = np.zeros((self.n_hidden, 1))\n",
        "        b2 = np.zeros((n_output, 1))\n",
        "\n",
        "        return W1, W2, b1, b2\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(z):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return expit(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(z):\n",
        "        \"\"\"Softmax activation function (for cross-entropy).\"\"\"\n",
        "        exp_values = np.exp(z - np.max(z, axis=0, keepdims=True))  # stability\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cross_entropy_loss(Y_enc, A3):\n",
        "        \"\"\"Cross-entropy loss function.\"\"\"\n",
        "        m = Y_enc.shape[1]\n",
        "        cost = -np.sum(Y_enc * np.log(A3 + 1e-8)) / m  # add small epsilon for stability\n",
        "        return cost\n",
        "\n",
        "    def _cost(self, A3, Y_enc, W1, W2):\n",
        "        \"\"\"Compute the cost function with regularization.\"\"\"\n",
        "        cost = self._cross_entropy_loss(Y_enc, A3)\n",
        "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
        "        return cost + L2_term\n",
        "\n",
        "    def _L2_reg(self, lambda_, W1, W2):\n",
        "        \"\"\"Compute L2 regularization cost.\"\"\"\n",
        "        return (lambda_ / 2.0) * (np.sum(W1[:, 1:] ** 2) + np.sum(W2[:, 1:] ** 2))\n",
        "\n",
        "    def _feedforward(self, X, W1, W2, b1, b2):\n",
        "        \"\"\"Compute the feedforward step.\"\"\"\n",
        "        A1 = self._add_bias_unit(X)\n",
        "        Z1 = W1 @ A1.T + b1\n",
        "        A2 = self._sigmoid(Z1)\n",
        "        A2 = self._add_bias_unit(A2.T)  # Add bias to hidden layer\n",
        "        Z2 = W2 @ A2.T + b2\n",
        "        A3 = self._softmax(Z2)  # Softmax for classification\n",
        "        return A1, Z1, A2, Z2, A3\n",
        "\n",
        "    def _add_bias_unit(self, X):\n",
        "        \"\"\"Add bias unit to the input data.\"\"\"\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        return np.hstack((ones, X))\n",
        "\n",
        "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
        "        \"\"\"Compute gradient step using backpropagation.\"\"\"\n",
        "        V2 = A3 - Y_enc  # For cross-entropy loss\n",
        "\n",
        "        # FIX: Transpose the result of matrix multiplication to match A2's shape\n",
        "        # Original: V1 = A2 * (1 - A2) * (W2[:, 1:].T @ V2)\n",
        "        delta = W2[:, 1:].T @ V2\n",
        "        # A2 shape is (samples, hidden+1), delta shape is (hidden, samples)\n",
        "        # We need to transpose delta to match A2's shape\n",
        "        delta_t = delta.T  # Now shape is (samples, hidden)\n",
        "\n",
        "        # We need to remove the bias column from A2 for element-wise multiplication\n",
        "        A2_nobias = A2[:, 1:]  # Remove bias column, now shape is (samples, hidden)\n",
        "        V1 = A2_nobias * (1 - A2_nobias) * delta_t\n",
        "\n",
        "        # Now calculate gradients with correct shapes\n",
        "        gradW2 = V2 @ A2\n",
        "        gradW1 = V1.T @ A1\n",
        "        gradb2 = np.sum(V2, axis=1).reshape((-1, 1))\n",
        "        gradb1 = np.sum(V1.T, axis=1).reshape((-1, 1))\n",
        "\n",
        "        # Regularize weights that are not bias terms\n",
        "        gradW1[:, 1:] += self.l2_C * W1[:, 1:]\n",
        "        gradW2[:, 1:] += self.l2_C * W2[:, 1:]\n",
        "\n",
        "        return gradW1, gradW2, gradb1, gradb2\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the model using mini-batches and update weights.\"\"\"\n",
        "        Y_enc = self._encode_labels(y)\n",
        "        n_features = X.shape[1]\n",
        "        n_output = Y_enc.shape[0]\n",
        "\n",
        "        # Initialize weights\n",
        "        W1, W2, b1, b2 = self._initialize_weights(n_features, n_output)\n",
        "\n",
        "        cost_history = []\n",
        "        for epoch in range(self.epochs):\n",
        "            minibatch_cost = 0\n",
        "\n",
        "            # Shuffle and split the data into mini-batches\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_enc_shuffled = Y_enc[:, permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], self.minibatch_size):\n",
        "                X_batch = X_shuffled[i:i+self.minibatch_size]\n",
        "                Y_batch = Y_enc_shuffled[:, i:i+self.minibatch_size]\n",
        "\n",
        "                # Feedforward\n",
        "                A1, Z1, A2, Z2, A3 = self._feedforward(X_batch, W1, W2, b1, b2)\n",
        "\n",
        "                # Compute cost\n",
        "                minibatch_cost = self._cost(A3, Y_batch, W1, W2)\n",
        "\n",
        "                # Backpropagation\n",
        "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1, A2, A3, Z1, Z2, Y_batch, W1, W2)\n",
        "\n",
        "                # Update weights and biases\n",
        "                W1 -= self.eta * gradW1\n",
        "                W2 -= self.eta * gradW2\n",
        "                b1 -= self.eta * gradb1\n",
        "                b2 -= self.eta * gradb2\n",
        "\n",
        "            cost_history.append(minibatch_cost)\n",
        "\n",
        "        self.W1, self.W2, self.b1, self.b2 = W1, W2, b1, b2\n",
        "        self.cost_ = cost_history\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels.\"\"\"\n",
        "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
        "        return np.argmax(A3, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844a71ff",
      "metadata": {
        "id": "844a71ff"
      },
      "outputs": [],
      "source": [
        "# raw data\n",
        "tlp_raw = TwoLayerPerceptron(n_hidden=30, epochs=100, eta=0.001, minibatch_size=64)\n",
        "tlp_raw.fit(X_train.values, y_train.values)\n",
        "\n",
        "# evaluate on test set\n",
        "y_pred = tlp_raw.predict(X_test.values)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "plt.plot(range(len(tlp_raw.cost_)), tlp_raw.cost_)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost (Loss)\")\n",
        "plt.title(\"Loss Function vs Epochs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a11ea0",
      "metadata": {
        "id": "41a11ea0"
      },
      "source": [
        "Based off of the graph above, the values vary between 1.35 and 1.42. The small learning rate and random mini-batch sampling might have likely produced the oscillations. I believe with further tuning with normalization and one-hot encoded is required to achieve stable results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5a32e37",
      "metadata": {
        "id": "b5a32e37"
      },
      "source": [
        "2.3 Now normalize the continuous numeric feature data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e52f67",
      "metadata": {
        "id": "a7e52f67"
      },
      "outputs": [],
      "source": [
        "# trying to normalize data\n",
        "scaler = StandardScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "# train and evaluate\n",
        "tlp_norm = TwoLayerPerceptron(n_hidden=30, epochs=100, eta=0.001, minibatch_size=64)\n",
        "tlp_norm.fit(X_train_norm, y_train)\n",
        "\n",
        "# predict and evaluate\n",
        "y_train_pred_norm = tlp_norm.predict(X_train_norm)\n",
        "y_test_pred_norm = tlp_norm.predict(X_test_norm)\n",
        "\n",
        "# calculate accuracy\n",
        "train_acc_norm = accuracy_score(y_train, y_train_pred_norm)\n",
        "test_acc_norm = accuracy_score(y_test, y_test_pred_norm)\n",
        "\n",
        "print(f\"Training Accuracy (Normalized): {train_acc_norm}\")\n",
        "print(f\"Test Accuracy (Normalized): {test_acc_norm}\")\n",
        "\n",
        "plt.plot(range(len(tlp_norm.cost_)), tlp_norm.cost_)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost (Loss)\")\n",
        "plt.title(\"Loss Function vs Epochs (Normalized Data)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d60206",
      "metadata": {
        "id": "d3d60206"
      },
      "source": [
        "With the data being normalized, we can see a strong downward trend, starting from 0.6 and dropping within the first 10-20 epochs. There's some loss that stabilizes near 0.05 after around 30 epochs. There aer some small fluctuations after convergence due to the mini-batch stochastic gradient descent, but this is normal. This is definitely an improvement from the previous model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3624f3",
      "metadata": {
        "id": "8a3624f3"
      },
      "source": [
        "2.4 One hot encode the categorical feature data. Use the example two-layer perceptron network from the class example and quantify performance using accuracy. Be sure that training converges by graphing the loss function versus the number of epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21aee97",
      "metadata": {
        "id": "f21aee97"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding categorical features\n",
        "X_combined = pd.concat([X_train, X_test])\n",
        "X_combined_encoded = pd.get_dummies(X_combined)\n",
        "\n",
        "# split back into train and test sets\n",
        "X_train_onehot = X_combined_encoded.iloc[:len(X_train_balanced)]\n",
        "X_test_onehot = X_combined_encoded.iloc[len(X_train_balanced):]\n",
        "\n",
        "# normalize one-hot encoded data\n",
        "scaler_onehot = StandardScaler()\n",
        "X_train_onehot_norm = scaler_onehot.fit_transform(X_train_onehot)\n",
        "X_test_onehot_norm = scaler_onehot.transform(X_test_onehot)\n",
        "\n",
        "# training model with normalized + one hot encoded data\n",
        "tlp_onehot = TwoLayerPerceptron(n_hidden=30, epochs=100, eta=0.001, minibatch_size=64)\n",
        "tlp_onehot.fit(X_train_onehot_norm, y_train_balanced)\n",
        "\n",
        "# evaluating the model\n",
        "y_train_pred_onehot = tlp_onehot.predict(X_train_onehot_norm)\n",
        "y_test_pred_onehot = tlp_onehot.predict(X_test_onehot_norm)\n",
        "\n",
        "# calculating accuracies for both training and testing sets\n",
        "train_acc_onehot = accuracy_score(y_train_balanced, y_train_pred_onehot)\n",
        "test_acc_onehot = accuracy_score(y_test, y_test_pred_onehot)\n",
        "\n",
        "print(f\"Training Accuracy (One-Hot Encoded + Normalized): {train_acc_onehot}\")\n",
        "print(f\"Test Accuracy (One-Hot Encoded + Normalized): {test_acc_onehot}\")\n",
        "\n",
        "plt.plot(range(len(tlp_onehot.cost_)), tlp_onehot.cost_)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost (Loss)\")\n",
        "plt.title(\"Loss Function vs Epochs (One-Hot Encoded + Normalized Data)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d30b916",
      "metadata": {
        "id": "7d30b916"
      },
      "source": [
        "The final model with normalized AND one-hot encoded data looks more or as stable as the previous model with only normalized data. The training loss shows a steep initial drop and stabilizes near zero after around 30 epochs, indicating that the model successfully converged and learned the relationships between the features efficiently. Based off of the accuracy scores, the final model is the most accurate compared to the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbbe2379",
      "metadata": {
        "id": "bbbe2379"
      },
      "source": [
        "2.4 Compare the performance of the three models you just trained. Are there any meaningful differences in performance? Explain, in your own words, why these models have (or do not have) different performances."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10781f2d",
      "metadata": {
        "id": "10781f2d"
      },
      "source": [
        "The preprocessing steps have a meaningful impact on model performance. The unnormalized model failed to converge, while normalization allowed the network to train successfully. Adding one-hot encoding further ensured that categorical variables were treated correctly, leading to efficient and stable learning. Overall, data preprocessing was critical for the perceptronâ€™s convergence and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae954499",
      "metadata": {
        "id": "ae954499"
      },
      "outputs": [],
      "source": [
        "# adapted from Copilot to compare accuracies\n",
        "\n",
        "acc_raw = accuracy_score(y_test, tlp_raw.predict(X_test))\n",
        "acc_norm = accuracy_score(y_test, tlp_norm.predict(X_test))\n",
        "acc_onehot = accuracy_score(y_test, tlp_onehot.predict(X_test))\n",
        "\n",
        "print(f\"Raw Data Accuracy: {acc_raw:.4f}\")\n",
        "print(f\"Normalized Data Accuracy: {acc_norm:.4f}\")\n",
        "print(f\"One-Hot + Normalized Accuracy: {acc_onehot:.4f}\")\n",
        "\n",
        "plt.bar([\"Raw\", \"Normalized\", \"One-Hot+Norm\"], [acc_raw, acc_norm, acc_onehot])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0de9357",
      "metadata": {
        "id": "b0de9357"
      },
      "source": [
        "*Use one-hot encoding and normalization on the dataset for the remainder of this lab assignment.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd095b3",
      "metadata": {
        "id": "0dd095b3"
      },
      "source": [
        "The bar graph above compares the accuracy of the three models. The normalized and one-hot encoded + normalized models achieved slightly higher accuracies (~25.5%) than the raw data model (~24.5%). While the improvement is relatively small, it confirms that preprocessing, particularly normalization, which contributes to more stable and effective learning. The results suggest that proper feature scaling and encoding help optimize model performance, even if the gains appear marginal for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a08b4eff",
      "metadata": {
        "id": "a08b4eff"
      },
      "source": [
        "### **3. Modeling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021f78ef",
      "metadata": {
        "id": "021f78ef"
      },
      "source": [
        "3.1 Add support for a third layer in the multi-layer perceptron. Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation). For magnitude calculation, you are free to use either the average absolute values or the L1/L2 norm.\n",
        "- Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3209c2ee",
      "metadata": {
        "id": "3209c2ee"
      },
      "outputs": [],
      "source": [
        "class ThreeLayerPerceptron(object):\n",
        "    def __init__(self, n_hidden1=30, n_hidden2=20, C=0.0, epochs=500, eta=0.001, minibatch_size=64, random_state=None):\n",
        "        np.random.seed(random_state)\n",
        "        self.n_hidden1 = n_hidden1\n",
        "        self.n_hidden2 = n_hidden2\n",
        "        self.l2_C = C\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.minibatch_size = minibatch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_labels(y):\n",
        "        \"\"\"Encode labels into one-hot representation.\"\"\"\n",
        "        return pd.get_dummies(y).values.T\n",
        "\n",
        "    def initialize_weights(self, n_features, n_output):\n",
        "        \"\"\"Initialize weights using Glorot initialization.\"\"\"\n",
        "        limit_1 = np.sqrt(6 / (n_features + self.n_hidden1))\n",
        "        limit_2 = np.sqrt(6 / (self.n_hidden1 + self.n_hidden2))\n",
        "        limit_3 = np.sqrt(6 / (self.n_hidden2 + n_output))\n",
        "\n",
        "        W1 = np.random.uniform(-limit_1, limit_1, (self.n_hidden1, n_features + 1))\n",
        "        W2 = np.random.uniform(-limit_2, limit_2, (self.n_hidden2, self.n_hidden1 + 1))\n",
        "        W3 = np.random.uniform(-limit_3, limit_3, (n_output, self.n_hidden2 + 1))\n",
        "\n",
        "        b1 = np.zeros((self.n_hidden1, 1))\n",
        "        b2 = np.zeros((self.n_hidden2, 1))\n",
        "        b3 = np.zeros((n_output, 1))\n",
        "\n",
        "        return W1, W2, W3, b1, b2, b3\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(z):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return expit(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(z):\n",
        "        \"\"\"Softmax activation function (for cross-entropy).\"\"\"\n",
        "        exp_values = np.exp(z - np.max(z, axis=0, keepdims=True))  # stability\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _cross_entropy_loss(Y_enc, A4):\n",
        "        \"\"\"Cross-entropy loss function.\"\"\"\n",
        "        m = Y_enc.shape[1]\n",
        "        cost = -np.sum(Y_enc * np.log(A4 + 1e-8)) / m  # add small epsilon for stability\n",
        "        return cost\n",
        "\n",
        "    def _cost(self, A4, Y_enc, W1, W2, W3):\n",
        "        \"\"\"Compute the cost function with regularization.\"\"\"\n",
        "        cost = self._cross_entropy_loss(Y_enc, A4)\n",
        "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3)\n",
        "        return cost + L2_term\n",
        "\n",
        "    def _L2_reg(self, lambda_, W1, W2, W3):\n",
        "        \"\"\"Compute L2 regularization cost.\"\"\"\n",
        "        return (lambda_ / 2.0) * (np.sum(W1[:, 1:] ** 2) + np.sum(W2[:, 1:] ** 2) + np.sum(W3[:, 1:] ** 2))\n",
        "\n",
        "    def _feedforward(self, X, W1, W2, W3, b1, b2, b3):\n",
        "        \"\"\"Compute the feedforward step.\"\"\"\n",
        "        A1 = self._add_bias_unit(X)\n",
        "        Z1 = W1 @ A1.T + b1\n",
        "        A2 = self._sigmoid(Z1)\n",
        "        A2 = self._add_bias_unit(A2.T)\n",
        "        Z2 = W2 @ A2.T + b2\n",
        "        A3 = self._sigmoid(Z2)\n",
        "        A3 = self._add_bias_unit(A3.T)\n",
        "        Z3 = W3 @ A3.T + b3\n",
        "        A4 = self._softmax(Z3)\n",
        "        return A1, Z1, A2, Z2, A3, Z3, A4\n",
        "\n",
        "    def _add_bias_unit(self, X):\n",
        "        \"\"\"Add bias unit to the input data.\"\"\"\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        return np.hstack((ones, X))\n",
        "\n",
        "    def _get_gradient(self, A1, A2, A3, A4, Z1, Z2, Z3, Y_enc, W1, W2, W3):\n",
        "        \"\"\"Compute gradient step using backpropagation.\"\"\"\n",
        "        V3 = A4 - Y_enc\n",
        "        delta3 = W3[:, 1:].T @ V3\n",
        "        delta3_t = delta3.T\n",
        "        A3_nobias = A3[:, 1:]\n",
        "        V2 = A3_nobias * (1 - A3_nobias) * delta3_t\n",
        "\n",
        "        delta2 = W2[:, 1:].T @ V2.T\n",
        "        delta2_t = delta2.T\n",
        "        A2_nobias = A2[:, 1:]\n",
        "        V1 = A2_nobias * (1 - A2_nobias) * delta2_t\n",
        "\n",
        "        gradW3 = V3 @ A3\n",
        "        gradW2 = V2.T @ A2\n",
        "        gradW1 = V1.T @ A1\n",
        "\n",
        "        gradb3 = np.sum(V3, axis=1).reshape((-1, 1))\n",
        "        gradb2 = np.sum(V2.T, axis=1).reshape((-1, 1))\n",
        "        gradb1 = np.sum(V1.T, axis=1).reshape((-1, 1))\n",
        "\n",
        "        gradW1[:, 1:] += self.l2_C * W1[:, 1:]\n",
        "        gradW2[:, 1:] += self.l2_C * W2[:, 1:]\n",
        "        gradW3[:, 1:] += self.l2_C * W3[:, 1:]\n",
        "\n",
        "        return gradW1, gradW2, gradW3, gradb1, gradb2, gradb3\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        Y_enc = self._encode_labels(y)\n",
        "        n_features = X.shape[1]\n",
        "        n_output = Y_enc.shape[0]\n",
        "\n",
        "        W1, W2, W3, b1, b2, b3 = self.initialize_weights(n_features, n_output)\n",
        "        cost_history = []\n",
        "        grad_mag1, grad_mag2, grad_mag3 = [], [], []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            minibatch_cost = 0\n",
        "            epoch_grad1, epoch_grad2, epoch_grad3 = [], [], []\n",
        "\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_enc_shuffled = Y_enc[:, permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], self.minibatch_size):\n",
        "                X_batch = X_shuffled[i:i+self.minibatch_size]\n",
        "                Y_batch = Y_enc_shuffled[:, i:i+self.minibatch_size]\n",
        "\n",
        "                A1, Z1, A2, Z2, A3, Z3, A4 = self._feedforward(X_batch, W1, W2, W3, b1, b2, b3)\n",
        "                minibatch_cost = self._cost(A4, Y_batch, W1, W2, W3)\n",
        "\n",
        "                gradW1, gradW2, gradW3, gradb1, gradb2, gradb3 = self._get_gradient(\n",
        "                    A1, A2, A3, A4, Z1, Z2, Z3, Y_batch, W1, W2, W3\n",
        "                )\n",
        "\n",
        "                # Track gradient magnitudes per minibatch\n",
        "                epoch_grad1.append(np.linalg.norm(gradW1))\n",
        "                epoch_grad2.append(np.linalg.norm(gradW2))\n",
        "                epoch_grad3.append(np.linalg.norm(gradW3))\n",
        "\n",
        "                # Update parameters\n",
        "                W1 -= self.eta * gradW1\n",
        "                W2 -= self.eta * gradW2\n",
        "                W3 -= self.eta * gradW3\n",
        "                b1 -= self.eta * gradb1\n",
        "                b2 -= self.eta * gradb2\n",
        "                b3 -= self.eta * gradb3\n",
        "\n",
        "            # Record average gradient magnitude for each epoch\n",
        "            grad_mag1.append(np.mean(epoch_grad1))\n",
        "            grad_mag2.append(np.mean(epoch_grad2))\n",
        "            grad_mag3.append(np.mean(epoch_grad3))\n",
        "            cost_history.append(minibatch_cost)\n",
        "\n",
        "        # Store results\n",
        "        self.W1, self.W2, self.W3 = W1, W2, W3\n",
        "        self.b1, self.b2, self.b3 = b1, b2, b3\n",
        "        self.cost_ = cost_history\n",
        "        self.grad_mag1_, self.grad_mag2_, self.grad_mag3_ = grad_mag1, grad_mag2, grad_mag3\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, _, _, _, _, _, A4 = self._feedforward(X, self.W1, self.W2, self.W3, self.b1, self.b2, self.b3)\n",
        "        return np.argmax(A4, axis=0)\n",
        "\n",
        "    def plot_gradient_flow(self):\n",
        "        plt.figure(figsize=(10, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(len(self.cost_)), self.cost_, label=\"Loss\", color='tab:blue')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Cost (Loss)\")\n",
        "        plt.title(\"Loss Function vs Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        epochs = range(len(self.grad_mag1_))\n",
        "        plt.plot(epochs, self.grad_mag1_, label='W1', color='tab:blue')\n",
        "        plt.plot(epochs, self.grad_mag2_, label='W2', color='tab:orange')\n",
        "        plt.plot(epochs, self.grad_mag3_, label='W3', color='tab:green')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Average Gradient Magnitudes\")\n",
        "        plt.title(\"Gradient Magnitudes During Training\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def train_evaluate_model(X_train, y_train, X_test, y_test, n_hidden1, n_hidden2, **params):\n",
        "    model = ThreeLayerPerceptron(n_hidden1=n_hidden1, n_hidden2=n_hidden2, epochs=100, eta=0.001, minibatch_size=64)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc}\")\n",
        "    print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "    model.plot_gradient_flow()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d53708",
      "metadata": {
        "id": "f7d53708"
      },
      "outputs": [],
      "source": [
        "params = {'epochs': 500,\n",
        "          'eta': 0.001,\n",
        "          'minibatch_size': 64,\n",
        "          'C': 0.01,\n",
        "          'random_state': 42,\n",
        "}\n",
        "\n",
        "print(\"Training and evaluating Three-Layer Perceptron with 30 and 20 hidden units:\")\n",
        "hidden_3 = [30,20]\n",
        "three_layer_model = train_evaluate_model(X_train_onehot_norm, y_train_balanced, X_test_onehot_norm, y_test, hidden_3[0], hidden_3[1], **params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8e9c26-c873-422c-95d7-962050d74624",
      "metadata": {
        "id": "4e8e9c26-c873-422c-95d7-962050d74624"
      },
      "source": [
        "Based on the \"Gradient Magnitudes During Training\" graph above, the average gradient magnitudes differ greatly for each layer of the perceptron. For the first layer of the perceptron, the magnitudes start relatively low and then increase tremendously as the number of epochs also increases. This means that in the first layer, the model makes very large changes when updating the weights, which could potentially raise issues and result in exploding gradients. For the second layer, there is little to no increase in the gradient magnitudes since they appear to stay at around 7 for all of the epochs. The third layer had almost the exact opposite effect on the gradient magnitudes compared to the first layer. For this layer, the gradient magnitudes started off moderate to high and decreased sharply as the epochs increased before evening out at around 2 or 3 for the gradient magnitudes. Additionally, the loss for the three-layer perceptron was very similar to the two-layer perceptron, seeing as how both models started with a high cost at zero epochs and then dropped dramatically before leveling off at around 0 and 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae7841f",
      "metadata": {
        "id": "2ae7841f"
      },
      "source": [
        "3.2 Repeat the previous step, adding support for a fourth layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c840eb2-c74a-433d-8ae3-9d922f976b30",
      "metadata": {
        "id": "3c840eb2-c74a-433d-8ae3-9d922f976b30"
      },
      "outputs": [],
      "source": [
        "class FourLayerPerceptron(ThreeLayerPerceptron):\n",
        "    def __init__(self, n_hidden1=30, n_hidden2=20, n_hidden3=10, C=0.0, epochs=500, eta=0.001, minibatch_size=64, random_state=None):\n",
        "            np.random.seed(random_state)\n",
        "            self.n_hidden1 = n_hidden1\n",
        "            self.n_hidden2 = n_hidden2\n",
        "            self.n_hidden3 = n_hidden3\n",
        "            self.l2_C = C\n",
        "            self.epochs = epochs\n",
        "            self.eta = eta\n",
        "            self.minibatch_size = minibatch_size\n",
        "\n",
        "    def initialize_weights(self, n_features, n_output):\n",
        "        \"\"\"Initialize weights using Glorot initialization.\"\"\"\n",
        "        limit_1 = np.sqrt(6 / (n_features + self.n_hidden1))\n",
        "        limit_2 = np.sqrt(6 / (self.n_hidden1 + self.n_hidden2))\n",
        "        limit_3 = np.sqrt(6 / (self.n_hidden2 + self.n_hidden3))\n",
        "        limit_4 = np.sqrt(6 / (self.n_hidden3 + n_output))\n",
        "\n",
        "        W1 = np.random.uniform(-limit_1, limit_1, (self.n_hidden1, n_features + 1))\n",
        "        W2 = np.random.uniform(-limit_2, limit_2, (self.n_hidden2, self.n_hidden1 + 1))\n",
        "        W3 = np.random.uniform(-limit_3, limit_3, (self.n_hidden3, self.n_hidden2 + 1))\n",
        "        W4 = np.random.uniform(-limit_4, limit_4, (n_output, self.n_hidden3 + 1))\n",
        "\n",
        "        b1 = np.zeros((self.n_hidden1, 1))\n",
        "        b2 = np.zeros((self.n_hidden2, 1))\n",
        "        b3 = np.zeros((self.n_hidden3, 1))\n",
        "        b4 = np.zeros((n_output, 1))\n",
        "\n",
        "        return W1, W2, W3, W4, b1, b2, b3, b4\n",
        "\n",
        "    @staticmethod\n",
        "    def _cross_entropy_loss(Y_enc, A5):\n",
        "        \"\"\"Cross-entropy loss function.\"\"\"\n",
        "        m = Y_enc.shape[1]\n",
        "        cost = -np.sum(Y_enc * np.log(A5 + 1e-8)) / m\n",
        "        return cost\n",
        "\n",
        "    def _cost(self, A5, Y_enc, W1, W2, W3, W4):\n",
        "        \"\"\"Compute the cost function with regularization.\"\"\"\n",
        "        cost = self._cross_entropy_loss(Y_enc, A5)\n",
        "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4)\n",
        "        return cost + L2_term\n",
        "\n",
        "    def _L2_reg(self, lambda_, W1, W2, W3, W4):\n",
        "        \"\"\"Compute L2 regularization cost.\"\"\"\n",
        "        return (lambda_ / 2.0) * (np.sum(W1[:, 1:] ** 2) + np.sum(W2[:, 1:] ** 2) + np.sum(W3[:, 1:] ** 2)\n",
        "                                  + np.sum(W4[:, 1:] ** 2))\n",
        "\n",
        "    def _feedforward(self, X, W1, W2, W3, W4, b1, b2, b3, b4):\n",
        "        \"\"\"Compute the feedforward step.\"\"\"\n",
        "        A1 = self._add_bias_unit(X)\n",
        "        Z1 = W1 @ A1.T + b1\n",
        "        A2 = self._sigmoid(Z1)\n",
        "        A2 = self._add_bias_unit(A2.T)\n",
        "        Z2 = W2 @ A2.T + b2\n",
        "        A3 = self._sigmoid(Z2)\n",
        "        A3 = self._add_bias_unit(A3.T)\n",
        "        Z3 = W3 @ A3.T + b3\n",
        "        A4 = self._sigmoid(Z3)\n",
        "        A4 = self._add_bias_unit(A4.T)\n",
        "        Z4 = W4 @ A4.T + b4\n",
        "        A5 = self._softmax(Z4)\n",
        "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5\n",
        "\n",
        "    def _get_gradient(self, A1, A2, A3, A4, A5, Z1, Z2, Z3, Z4, Y_enc, W1, W2, W3, W4):\n",
        "        \"\"\"Compute gradient step using backpropagation.\"\"\"\n",
        "        V4 = A5 - Y_enc\n",
        "        delta4 = W4[:, 1:].T @ V4\n",
        "        delta4_t = delta4.T\n",
        "        A4_nobias = A4[:, 1:]\n",
        "        V3 = A4_nobias * (1 - A4_nobias) * delta4_t\n",
        "\n",
        "        delta3 = W3[:, 1:].T @ V3.T\n",
        "        delta3_t = delta3.T\n",
        "        A3_nobias = A3[:, 1:]\n",
        "        V2 = A3_nobias * (1 - A3_nobias) * delta3_t\n",
        "\n",
        "        delta2 = W2[:, 1:].T @ V2.T\n",
        "        delta2_t = delta2.T\n",
        "        A2_nobias = A2[:, 1:]\n",
        "        V1 = A2_nobias * (1 - A2_nobias) * delta2_t\n",
        "\n",
        "        gradW4 = V4 @ A4\n",
        "        gradW3 = V3.T @ A3\n",
        "        gradW2 = V2.T @ A2\n",
        "        gradW1 = V1.T @ A1\n",
        "\n",
        "        gradb4 = np.sum(V4, axis=1).reshape((-1, 1))\n",
        "        gradb3 = np.sum(V3.T, axis=1).reshape((-1, 1))\n",
        "        gradb2 = np.sum(V2.T, axis=1).reshape((-1, 1))\n",
        "        gradb1 = np.sum(V1.T, axis=1).reshape((-1, 1))\n",
        "\n",
        "        gradW1[:, 1:] += self.l2_C * W1[:, 1:]\n",
        "        gradW2[:, 1:] += self.l2_C * W2[:, 1:]\n",
        "        gradW3[:, 1:] += self.l2_C * W3[:, 1:]\n",
        "        gradW4[:, 1:] += self.l2_C * W4[:, 1:]\n",
        "\n",
        "        return gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        Y_enc = self._encode_labels(y)\n",
        "        n_features = X.shape[1]\n",
        "        n_output = Y_enc.shape[0]\n",
        "\n",
        "        W1, W2, W3, W4, b1, b2, b3, b4 = self.initialize_weights(n_features, n_output)\n",
        "        cost_history = []\n",
        "        grad_mag1, grad_mag2, grad_mag3, grad_mag4 = [], [], [], []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            minibatch_cost = 0\n",
        "            epoch_grad1, epoch_grad2, epoch_grad3, epoch_grad4 = [], [], [], []\n",
        "\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_enc_shuffled = Y_enc[:, permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], self.minibatch_size):\n",
        "                X_batch = X_shuffled[i:i+self.minibatch_size]\n",
        "                Y_batch = Y_enc_shuffled[:, i:i+self.minibatch_size]\n",
        "\n",
        "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5 = self._feedforward(X_batch, W1, W2, W3, W4, b1, b2, b3, b4)\n",
        "                minibatch_cost = self._cost(A5, Y_batch, W1, W2, W3, W4)\n",
        "\n",
        "                gradW1, gradW2, gradW3, gradW4, gradb1, gradb2, gradb3, gradb4 = self._get_gradient(\n",
        "                    A1, A2, A3, A4, A5, Z1, Z2, Z3, Z4, Y_batch, W1, W2, W3, W4\n",
        "                )\n",
        "\n",
        "                # Track gradient magnitudes per minibatch\n",
        "                epoch_grad1.append(np.linalg.norm(gradW1))\n",
        "                epoch_grad2.append(np.linalg.norm(gradW2))\n",
        "                epoch_grad3.append(np.linalg.norm(gradW3))\n",
        "                epoch_grad4.append(np.linalg.norm(gradW4))\n",
        "\n",
        "                # Update parameters\n",
        "                W1 -= self.eta * gradW1\n",
        "                W2 -= self.eta * gradW2\n",
        "                W3 -= self.eta * gradW3\n",
        "                W4 -= self.eta * gradW4\n",
        "                b1 -= self.eta * gradb1\n",
        "                b2 -= self.eta * gradb2\n",
        "                b3 -= self.eta * gradb3\n",
        "                b4 -= self.eta * gradb4\n",
        "\n",
        "            # Record average gradient magnitude for each epoch\n",
        "            grad_mag1.append(np.mean(epoch_grad1))\n",
        "            grad_mag2.append(np.mean(epoch_grad2))\n",
        "            grad_mag3.append(np.mean(epoch_grad3))\n",
        "            grad_mag4.append(np.mean(epoch_grad4))\n",
        "            cost_history.append(minibatch_cost)\n",
        "\n",
        "        # Store results\n",
        "        self.W1, self.W2, self.W3, self.W4 = W1, W2, W3, W4\n",
        "        self.b1, self.b2, self.b3, self.b4 = b1, b2, b3, b4\n",
        "        self.cost_ = cost_history\n",
        "        self.grad_mag1_, self.grad_mag2_, self.grad_mag3_, self.grad_mag4_ = grad_mag1, grad_mag2, grad_mag3, grad_mag4\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, _, _, _, _, _, _, _, A5 = self._feedforward(X, self.W1, self.W2, self.W3, self.W4, self.b1,\n",
        "                                                 self.b2, self.b3, self.b4)\n",
        "        return np.argmax(A5, axis=0)\n",
        "\n",
        "    def plot_gradient_flow(self):\n",
        "        plt.figure(figsize=(10, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(len(self.cost_)), self.cost_, label=\"Loss\", color='tab:blue')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Cost (Loss)\")\n",
        "        plt.title(\"Loss Function vs Epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        epochs = range(len(self.grad_mag1_))\n",
        "        plt.plot(epochs, self.grad_mag1_, label='W1', color='tab:blue')\n",
        "        plt.plot(epochs, self.grad_mag2_, label='W2', color='tab:orange')\n",
        "        plt.plot(epochs, self.grad_mag3_, label='W3', color='tab:green')\n",
        "        plt.plot(epochs, self.grad_mag4_, label='W4', color='tab:red')\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Average Gradient Magnitudes\")\n",
        "        plt.title(\"Gradient Magnitudes During Training\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def train_evaluate_model(X_train, y_train, X_test, y_test, n_hidden1, n_hidden2, n_hidden3, **params):\n",
        "    model = FourLayerPerceptron(n_hidden1=n_hidden1, n_hidden2=n_hidden2, n_hidden3=n_hidden3,\n",
        "                                 epochs=100, eta=0.001, minibatch_size=64)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc}\")\n",
        "    print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "    model.plot_gradient_flow()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee4d3a2-0419-45e3-838c-a2bc2e3aaf41",
      "metadata": {
        "id": "9ee4d3a2-0419-45e3-838c-a2bc2e3aaf41"
      },
      "outputs": [],
      "source": [
        "print(\"Training and evaluating a Four-Layer Perceptron with 30, 20, and 10 hidden units:\")\n",
        "hidden_layers = [30,20,10]\n",
        "four_layer_model = train_evaluate_model(X_train_onehot_norm, y_train_balanced, X_test_onehot_norm,\n",
        "                                         y_test, hidden_layers[0], hidden_layers[1], hidden_layers[2], **params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db897e8b-bb8d-45b4-8283-e603cfeda7db",
      "metadata": {
        "id": "db897e8b-bb8d-45b4-8283-e603cfeda7db"
      },
      "source": [
        "For the four-layer perceptron model, the first layer performed quite similarly to that of the three-layer perceptron model, since the average gradient magnitudes of both started off small and increased greatly as the number of epochs increased. However, unlike with the three-layer perceptron, the four-layer perceptron's second layer saw a noticeable increase in the average gradient magnitudes from 0 to about 40 epochs before leveling off at a magnitude of roughly 15. Furthermore, the third layer's magnitudes did not see a large drop like with the three-layer perceptron, but instead saw a small increase from about 0 to 10 epochs before evening out and remaining at an average gradient magnitude of about 7 for the rest of the epochs. For the four-layer perceptron model, it was the fourth layer instead of the third layer that saw a large decrease in the average gradient magnitudes before converging to a magnitude of roughly 2. This is due to the cross entropy loss function using the deepest layer of the perceptron model to calculate the difference between the model's prediction and the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0245cdd8",
      "metadata": {
        "id": "0245cdd8"
      },
      "source": [
        "3.3 Repeat the previous step, adding support for a fifth layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c5d828-bdc6-44fc-8623-7c01354ec387",
      "metadata": {
        "id": "b9c5d828-bdc6-44fc-8623-7c01354ec387"
      },
      "outputs": [],
      "source": [
        "class FiveLayerPerceptron(FourLayerPerceptron):\n",
        "    def __init__(self, n_hidden1=30, n_hidden2=20, n_hidden3=10, n_hidden4=5,\n",
        "                 C=0.0, epochs=500, eta=0.001, minibatch_size=64, random_state=None):\n",
        "        np.random.seed(random_state)\n",
        "        self.n_hidden1 = n_hidden1\n",
        "        self.n_hidden2 = n_hidden2\n",
        "        self.n_hidden3 = n_hidden3\n",
        "        self.n_hidden4 = n_hidden4\n",
        "        self.l2_C = C\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.minibatch_size = minibatch_size\n",
        "\n",
        "    def initialize_weights(self, n_features, n_output):\n",
        "        # glorot initialization\n",
        "        limit_1 = np.sqrt(6 / (n_features + self.n_hidden1))\n",
        "        limit_2 = np.sqrt(6 / (self.n_hidden1 + self.n_hidden2))\n",
        "        limit_3 = np.sqrt(6 / (self.n_hidden2 + self.n_hidden3))\n",
        "        limit_4 = np.sqrt(6 / (self.n_hidden3 + self.n_hidden4))\n",
        "        limit_5 = np.sqrt(6 / (self.n_hidden4 + n_output))\n",
        "\n",
        "        W1 = np.random.uniform(-limit_1, limit_1, (self.n_hidden1, n_features + 1))\n",
        "        W2 = np.random.uniform(-limit_2, limit_2, (self.n_hidden2, self.n_hidden1 + 1))\n",
        "        W3 = np.random.uniform(-limit_3, limit_3, (self.n_hidden3, self.n_hidden2 + 1))\n",
        "        W4 = np.random.uniform(-limit_4, limit_4, (self.n_hidden4, self.n_hidden3 + 1))\n",
        "        W5 = np.random.uniform(-limit_5, limit_5, (n_output, self.n_hidden4 + 1))\n",
        "\n",
        "        b1 = np.zeros((self.n_hidden1, 1))\n",
        "        b2 = np.zeros((self.n_hidden2, 1))\n",
        "        b3 = np.zeros((self.n_hidden3, 1))\n",
        "        b4 = np.zeros((self.n_hidden4, 1))\n",
        "        b5 = np.zeros((n_output, 1))\n",
        "\n",
        "        return W1, W2, W3, W4, W5, b1, b2, b3, b4, b5\n",
        "\n",
        "    @staticmethod\n",
        "    def _cross_entropy_loss(Y_enc, A6):\n",
        "        \"\"\"Cross-entropy loss function.\"\"\"\n",
        "        m = Y_enc.shape[1]\n",
        "        cost = -np.sum(Y_enc * np.log(A6 + 1e-8)) / m\n",
        "        return cost\n",
        "\n",
        "    def _cost(self, A6, Y_enc, W1, W2, W3, W4, W5):\n",
        "        \"\"\"Compute the cost function with regularization.\"\"\"\n",
        "        cost = self._cross_entropy_loss(Y_enc, A6)\n",
        "        L2_term = self._L2_reg(self.l2_C, W1, W2, W3, W4, W5)\n",
        "        return cost + L2_term\n",
        "\n",
        "    def _L2_reg(self, lambda_, W1, W2, W3, W4, W5):\n",
        "        \"\"\"Compute L2 regularization cost.\"\"\"\n",
        "        return (lambda_ / 2.0) * (np.sum(W1[:, 1:] ** 2) + np.sum(W2[:, 1:] ** 2) + np.sum(W3[:, 1:] ** 2)\n",
        "                                  + np.sum(W4[:, 1:] ** 2) + np.sum(W5[:, 1:] ** 2))\n",
        "\n",
        "    def _feedforward(self, X, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5):\n",
        "        A1 = self._add_bias_unit(X)\n",
        "        Z1 = W1 @ A1.T + b1\n",
        "        A2 = self._sigmoid(Z1)\n",
        "        A2 = self._add_bias_unit(A2.T)\n",
        "        Z2 = W2 @ A2.T + b2\n",
        "        A3 = self._sigmoid(Z2)\n",
        "        A3 = self._add_bias_unit(A3.T)\n",
        "        Z3 = W3 @ A3.T + b3\n",
        "        A4 = self._sigmoid(Z3)\n",
        "        A4 = self._add_bias_unit(A4.T)\n",
        "        Z4 = W4 @ A4.T + b4\n",
        "        A5 = self._sigmoid(Z4)\n",
        "        A5 = self._add_bias_unit(A5.T)\n",
        "        Z5 = W5 @ A5.T + b5\n",
        "        A6 = self._softmax(Z5)\n",
        "        return A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6\n",
        "\n",
        "    def _get_gradient(self, A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5, Y_enc, W1, W2, W3, W4, W5):\n",
        "        # backprop\n",
        "        V5 = A6 - Y_enc\n",
        "        delta5 = W5[:, 1:].T @ V5\n",
        "        delta5_t = delta5.T\n",
        "        A5_nobias = A5[:, 1:]\n",
        "        V4 = A5_nobias * (1 - A5_nobias) * delta5_t\n",
        "\n",
        "        delta4 = W4[:, 1:].T @ V4.T\n",
        "        delta4_t = delta4.T\n",
        "        A4_nobias = A4[:, 1:]\n",
        "        V3 = A4_nobias * (1 - A4_nobias) * delta4_t\n",
        "\n",
        "        delta3 = W3[:, 1:].T @ V3.T\n",
        "        delta3_t = delta3.T\n",
        "        A3_nobias = A3[:, 1:]\n",
        "        V2 = A3_nobias * (1 - A3_nobias) * delta3_t\n",
        "\n",
        "        delta2 = W2[:, 1:].T @ V2.T\n",
        "        delta2_t = delta2.T\n",
        "        A2_nobias = A2[:, 1:]\n",
        "        V1 = A2_nobias * (1 - A2_nobias) * delta2_t\n",
        "        # gradients\n",
        "        gradW5 = V5 @ A5\n",
        "        gradW4 = V4.T @ A4\n",
        "        gradW3 = V3.T @ A3\n",
        "        gradW2 = V2.T @ A2\n",
        "        gradW1 = V1.T @ A1\n",
        "\n",
        "        gradb5 = np.sum(V5, axis=1).reshape((-1, 1))\n",
        "        gradb4 = np.sum(V4.T, axis=1).reshape((-1, 1))\n",
        "        gradb3 = np.sum(V3.T, axis=1).reshape((-1, 1))\n",
        "        gradb2 = np.sum(V2.T, axis=1).reshape((-1, 1))\n",
        "        gradb1 = np.sum(V1.T, axis=1).reshape((-1, 1))\n",
        "\n",
        "        gradW1[:, 1:] += self.l2_C * W1[:, 1:]\n",
        "        gradW2[:, 1:] += self.l2_C * W2[:, 1:]\n",
        "        gradW3[:, 1:] += self.l2_C * W3[:, 1:]\n",
        "        gradW4[:, 1:] += self.l2_C * W4[:, 1:]\n",
        "        gradW5[:, 1:] += self.l2_C * W5[:, 1:]\n",
        "\n",
        "        return gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        Y_enc = self._encode_labels(y)\n",
        "        n_features = X.shape[1]\n",
        "        n_output = Y_enc.shape[0]\n",
        "\n",
        "        W1, W2, W3, W4, W5, b1, b2, b3, b4, b5 = self.initialize_weights(n_features, n_output)\n",
        "        cost_history = []\n",
        "        grad_mag1, grad_mag2, grad_mag3, grad_mag4, grad_mag5 = [], [], [], [], []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            minibatch_cost = 0\n",
        "            g1, g2, g3, g4, g5 = [], [], [], [], []\n",
        "\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_enc_shuffled = Y_enc[:, permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], self.minibatch_size):\n",
        "                X_batch = X_shuffled[i:i+self.minibatch_size]\n",
        "                Y_batch = Y_enc_shuffled[:, i:i+self.minibatch_size]\n",
        "\n",
        "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6 = self._feedforward(\n",
        "                    X_batch, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5\n",
        "                )\n",
        "                minibatch_cost = self._cost(A6, Y_batch, W1, W2, W3, W4, W5)\n",
        "                grads = self._get_gradient(A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5,\n",
        "                                           Y_batch, W1, W2, W3, W4, W5)\n",
        "\n",
        "                gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5 = grads\n",
        "\n",
        "                # track gradient\n",
        "                g1.append(np.linalg.norm(gradW1))\n",
        "                g2.append(np.linalg.norm(gradW2))\n",
        "                g3.append(np.linalg.norm(gradW3))\n",
        "                g4.append(np.linalg.norm(gradW4))\n",
        "                g5.append(np.linalg.norm(gradW5))\n",
        "\n",
        "                # update weights\n",
        "                W1 -= self.eta * gradW1\n",
        "                W2 -= self.eta * gradW2\n",
        "                W3 -= self.eta * gradW3\n",
        "                W4 -= self.eta * gradW4\n",
        "                W5 -= self.eta * gradW5\n",
        "                b1 -= self.eta * gradb1\n",
        "                b2 -= self.eta * gradb2\n",
        "                b3 -= self.eta * gradb3\n",
        "                b4 -= self.eta * gradb4\n",
        "                b5 -= self.eta * gradb5\n",
        "\n",
        "            grad_mag1.append(np.mean(g1))\n",
        "            grad_mag2.append(np.mean(g2))\n",
        "            grad_mag3.append(np.mean(g3))\n",
        "            grad_mag4.append(np.mean(g4))\n",
        "            grad_mag5.append(np.mean(g5))\n",
        "            cost_history.append(minibatch_cost)\n",
        "\n",
        "        # store everything\n",
        "        self.W1, self.W2, self.W3, self.W4, self.W5 = W1, W2, W3, W4, W5\n",
        "        self.b1, self.b2, self.b3, self.b4, self.b5 = b1, b2, b3, b4, b5\n",
        "        self.cost_ = cost_history\n",
        "        self.grad_mag1_, self.grad_mag2_, self.grad_mag3_, self.grad_mag4_, self.grad_mag5_ = (\n",
        "            grad_mag1, grad_mag2, grad_mag3, grad_mag4, grad_mag5\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, _, _, _, _, _, _, _, _, _, A6 = self._feedforward(\n",
        "            X, self.W1, self.W2, self.W3, self.W4, self.W5,\n",
        "            self.b1, self.b2, self.b3, self.b4, self.b5\n",
        "        )\n",
        "        return np.argmax(A6, axis=0)\n",
        "\n",
        "    def plot_gradient_flow(self):\n",
        "        plt.figure(figsize=(10, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(len(self.cost_)), self.cost_, label=\"loss\", color='tab:blue')\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"cost (loss)\")\n",
        "        plt.title(\"loss function vs epochs\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        epochs = range(len(self.grad_mag1_))\n",
        "        plt.plot(epochs, self.grad_mag1_, label='W1')\n",
        "        plt.plot(epochs, self.grad_mag2_, label='W2')\n",
        "        plt.plot(epochs, self.grad_mag3_, label='W3')\n",
        "        plt.plot(epochs, self.grad_mag4_, label='W4')\n",
        "        plt.plot(epochs, self.grad_mag5_, label='W5')\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"average gradient magnitudes\")\n",
        "        plt.title(\"gradient magnitudes during training\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def train_evaluate_model(X_train, y_train, X_test, y_test, n_hidden1, n_hidden2, n_hidden3, n_hidden4, **params):\n",
        "    model = FiveLayerPerceptron(n_hidden1=n_hidden1, n_hidden2=n_hidden2,\n",
        "                                n_hidden3=n_hidden3, n_hidden4=n_hidden4,\n",
        "                                epochs=100, eta=0.001, minibatch_size=64)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "    model.plot_gradient_flow()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dddc120a-dd1a-486e-8226-b2ec5791b637",
      "metadata": {
        "id": "dddc120a-dd1a-486e-8226-b2ec5791b637"
      },
      "outputs": [],
      "source": [
        "print(\"Training and evaluating a Five-Layer Perceptron with 30, 20, 10 and 5 hidden units:\")\n",
        "hidden_layers = [30, 20, 10, 5]\n",
        "five_layer_model = train_evaluate_model(\n",
        "    X_train_onehot_norm, y_train_balanced,\n",
        "    X_test_onehot_norm, y_test,\n",
        "    hidden_layers[0], hidden_layers[1], hidden_layers[2], hidden_layers[3], **params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the five layer perceptron model, the first layer showed a really steep increase in the average gradient magnitudes within the first few epochs before leveling off at around 40. The second and third layers increased at smaller magnitudes, and reached values at around 10-20 at around 20 epochs and above. The fourth layer started off with higher gradients but decreased and flattened around a magnitude of 5. The fifth layer saw a drop early on before arriving at the lowest magnitude, around 3. The graph shows that as depth increases, the gradient magnitudes become smaller."
      ],
      "metadata": {
        "id": "0_3JdIDVS-x1"
      },
      "id": "0_3JdIDVS-x1"
    },
    {
      "cell_type": "markdown",
      "id": "4d0710a9",
      "metadata": {
        "id": "4d0710a9"
      },
      "source": [
        "3.4 Implement an adaptive learning technique that was discussed in lecture and use it on the five layer network (choose either RMSProp or AdaDelta). Discuss which adaptive method you chose. Compare the performance of your five layer model with and without the adaptive learning strategy. **Do not use AdaM for the adaptive learning technique as it is part of the exceptional work.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8459ea3a-e349-4082-94b2-fda3726342e9",
      "metadata": {
        "id": "8459ea3a-e349-4082-94b2-fda3726342e9"
      },
      "outputs": [],
      "source": [
        "class RMSPropFLP(FiveLayerPerceptron):\n",
        "    def __init__(self, alpha=0.1, **kwds):\n",
        "        super().__init__(**kwds)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _rmsprop_update(self, gradW1, gradW2, gradW3, gradW4, gradW5):\n",
        "        if not hasattr(self,'_V1_prev'):\n",
        "            self._V1_prev = np.zeros_like(gradW1)\n",
        "            self._V2_prev = np.zeros_like(gradW2)\n",
        "            self._V3_prev = np.zeros_like(gradW3)\n",
        "            self._V4_prev = np.zeros_like(gradW4)\n",
        "            self._V5_prev = np.zeros_like(gradW5)\n",
        "\n",
        "        G1 = gradW1*gradW1\n",
        "        G2 = gradW2*gradW2\n",
        "        G3 = gradW3*gradW3\n",
        "        G4 = gradW4*gradW4\n",
        "        G5 = gradW5*gradW5\n",
        "\n",
        "        V1 = self.alpha*self._V1_prev + (1-self.alpha)*G1 + 1e-7\n",
        "        V2 = self.alpha*self._V2_prev + (1-self.alpha)*G2 + 1e-7\n",
        "        V3 = self.alpha*self._V3_prev + (1-self.alpha)*G3 + 1e-7\n",
        "        V4 = self.alpha*self._V4_prev + (1-self.alpha)*G4 + 1e-7\n",
        "        V5 = self.alpha*self._V5_prev + (1-self.alpha)*G5 + 1e-7\n",
        "\n",
        "        gradW1 = gradW1/np.sqrt(V1)\n",
        "        gradW2 = gradW2/np.sqrt(V2)\n",
        "        gradW3 = gradW3/np.sqrt(V3)\n",
        "        gradW4 = gradW4/np.sqrt(V4)\n",
        "        gradW5 = gradW5/np.sqrt(V5)\n",
        "\n",
        "        # save previous updates\n",
        "        self._V1_prev, self._V2_prev, self._V3_prev, self._V4_prev, self._V5_prev = V1, V2, V3, V4, V5\n",
        "\n",
        "        return gradW1, gradW2, gradW3, gradW4, gradW5\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        Y_enc = self._encode_labels(y)\n",
        "        n_features = X.shape[1]\n",
        "        n_output = Y_enc.shape[0]\n",
        "\n",
        "        W1, W2, W3, W4, W5, b1, b2, b3, b4, b5 = self.initialize_weights(n_features, n_output)\n",
        "        cost_history = []\n",
        "        grad_mag1, grad_mag2, grad_mag3, grad_mag4, grad_mag5 = [], [], [], [], []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            minibatch_cost = 0\n",
        "            g1, g2, g3, g4, g5 = [], [], [], [], []\n",
        "\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_enc_shuffled = Y_enc[:, permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], self.minibatch_size):\n",
        "                X_batch = X_shuffled[i:i+self.minibatch_size]\n",
        "                Y_batch = Y_enc_shuffled[:, i:i+self.minibatch_size]\n",
        "\n",
        "                A1, Z1, A2, Z2, A3, Z3, A4, Z4, A5, Z5, A6 = self._feedforward(\n",
        "                    X_batch, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5\n",
        "                )\n",
        "                minibatch_cost = self._cost(A6, Y_batch, W1, W2, W3, W4, W5)\n",
        "                grads = self._get_gradient(A1, A2, A3, A4, A5, A6, Z1, Z2, Z3, Z4, Z5,\n",
        "                                           Y_batch, W1, W2, W3, W4, W5)\n",
        "\n",
        "                gradW1, gradW2, gradW3, gradW4, gradW5, gradb1, gradb2, gradb3, gradb4, gradb5 = grads\n",
        "                # updates weights using rmsprop\n",
        "                gradW1, gradW2, gradW3, gradW4, gradW5 = self._rmsprop_update(gradW1, gradW2, gradW3, gradW4, gradW5)\n",
        "\n",
        "                # track gradient\n",
        "                g1.append(np.linalg.norm(gradW1))\n",
        "                g2.append(np.linalg.norm(gradW2))\n",
        "                g3.append(np.linalg.norm(gradW3))\n",
        "                g4.append(np.linalg.norm(gradW4))\n",
        "                g5.append(np.linalg.norm(gradW5))\n",
        "\n",
        "                # update weights\n",
        "                W1 -= self.eta * gradW1\n",
        "                W2 -= self.eta * gradW2\n",
        "                W3 -= self.eta * gradW3\n",
        "                W4 -= self.eta * gradW4\n",
        "                W5 -= self.eta * gradW5\n",
        "                b1 -= self.eta * gradb1\n",
        "                b2 -= self.eta * gradb2\n",
        "                b3 -= self.eta * gradb3\n",
        "                b4 -= self.eta * gradb4\n",
        "                b5 -= self.eta * gradb5\n",
        "\n",
        "            grad_mag1.append(np.mean(g1))\n",
        "            grad_mag2.append(np.mean(g2))\n",
        "            grad_mag3.append(np.mean(g3))\n",
        "            grad_mag4.append(np.mean(g4))\n",
        "            grad_mag5.append(np.mean(g5))\n",
        "            cost_history.append(minibatch_cost)\n",
        "\n",
        "        # store everything\n",
        "        self.W1, self.W2, self.W3, self.W4, self.W5 = W1, W2, W3, W4, W5\n",
        "        self.b1, self.b2, self.b3, self.b4, self.b5 = b1, b2, b3, b4, b5\n",
        "        self.cost_ = cost_history\n",
        "        self.grad_mag1_, self.grad_mag2_, self.grad_mag3_, self.grad_mag4_, self.grad_mag5_ = (\n",
        "            grad_mag1, grad_mag2, grad_mag3, grad_mag4, grad_mag5\n",
        "        )\n",
        "        return self\n",
        "\n",
        "def train_evaluate_model(X_train, y_train, X_test, y_test, n_hidden1, n_hidden2, n_hidden3, n_hidden4, **params):\n",
        "    model = RMSPropFLP(n_hidden1=n_hidden1, n_hidden2=n_hidden2,\n",
        "                                n_hidden3=n_hidden3, n_hidden4=n_hidden4,\n",
        "                                alpha=0.1,epochs=100, eta=0.001, minibatch_size=64)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
        "    print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "    model.plot_gradient_flow()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8915c65d-3c3f-4172-9ba9-870a0a5c2b62",
      "metadata": {
        "id": "8915c65d-3c3f-4172-9ba9-870a0a5c2b62"
      },
      "outputs": [],
      "source": [
        "print(\"Implementation of RMSProp with the Five-Layer Perceptron:\")\n",
        "hidden_layers = [30, 20, 10, 5]\n",
        "rms = train_evaluate_model(X_train_onehot_norm, y_train_balanced, X_test_onehot_norm,\n",
        "                                         y_test, hidden_layers[0], hidden_layers[1],\n",
        "                           hidden_layers[2], hidden_layers[3], **params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e329e0-012a-42d8-8dc5-85104c1f5775",
      "metadata": {
        "id": "94e329e0-012a-42d8-8dc5-85104c1f5775"
      },
      "source": [
        "The adaptive method we chose was RMSProp because it gives us a way to mitigate the effects of vanishing gradients. As the model goes deeper into its layers, the average gradient magnitudes decrease and eventually result in vanishing gradients since models reach a point where the weights are remaining essentially the same for all epochs. However, with RMSProp, the learning rate changes with the magnitude of the gradients to ensure stability despite differences in the magnitudes, thereby reducing the effect of vanishing gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard five layer perceptron had a 0.9875 training accuracy and a 0.9827 test accuracy. On the other hand, RMSProp five layer perceptron had a 0.9889 training accuracy and a 0.9838 test accuracy. Both models did well, but RMSProp showed slight improvement in both the training and the test accuracy. Though the improvement is small, it's still meaningful since the models are very close to perfect classification. Looking at the magnitude plots, the standard model depicts the gradient magnitudes increase quickly but then flattens for each layer. On the otherhand, for RMSProp, the gradients are lower but they're more balanced across each layers."
      ],
      "metadata": {
        "id": "bkkWrrBSJN9h"
      },
      "id": "bkkWrrBSJN9h"
    },
    {
      "cell_type": "markdown",
      "id": "8ba3bd8f",
      "metadata": {
        "id": "8ba3bd8f"
      },
      "source": [
        "### **4. Exceptional Work**\n",
        "\n",
        "Implement adaptive momentum (AdaM) in the five layer neural network and quantify the performance compared to other methods.  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}